{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfs_to_image(directoryList):\n",
    "\tprint('converting pdf document to images')\n",
    "\tdirectoryList = os.listdir(folder)\n",
    "\t\n",
    "\tfor pdfPath in directoryList:\n",
    "\t\tprint(pdfPath)\n",
    "\t\tif pdfPath.endswith(\".pdf\"):\n",
    "\t\t\tos.chdir(folder)\n",
    "\t\t\tpages = convert_from_path(pdfPath)\n",
    "\t\t\tos.chdir(targetFolder)\n",
    "\t\t\tfor page in pages:\n",
    "\t\t\t\tpage.save(\"%s-page%d.tiff\" %(pdfPath,pages.index(page)), 'JPEG')\n",
    "\tprint('pdf to image conversion done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_text(directoryList):\n",
    "\tprint(\"Reading images.......!!!!\")\n",
    "\tdataFile=[]\n",
    "\tfor imPath in directoryList:\n",
    "\t\t\n",
    "\t\tprint(imPath)\n",
    "\n",
    "\t\t#imPath = \"/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/ab.pdf-page0.tiff\"\n",
    "\t\t# Define config parameters.\n",
    "\t\t# '-l eng'  for using the English language \n",
    "\t\t# '--oem 1' for using LSTM OCR Engine\n",
    "\t\tconfig = ('-l eng --oem 1 --psm 3')\n",
    "\t\t\n",
    "\t\t# Read image from disk\n",
    "\t\tim = cv2.imread(str(imPath), cv2.IMREAD_COLOR)\n",
    "\t\t# Run tesseract OCR on image\n",
    "\t\t#text=\"0\"\n",
    "\t\t#print(im)\n",
    "\t\ttext = pytesseract.image_to_string(im, config=config)\n",
    "\t\ttext = text.replace('\\n',' ')\n",
    "\t\ttext = text.replace('\\t',' ')\n",
    "\t\ttext= text.rstrip()\n",
    "\t\ttext= text.lstrip()\n",
    "\t\ttext = text.replace(' +',' ')\n",
    "\t\tdataFile.append(text)\n",
    "\t\t# Print recognized text\n",
    "\t\n",
    "\tprint('Printing data....')\t\n",
    "\tprint(dataFile)\n",
    "\tos.chdir('/home/ikscare/Documents/Projects/Mousam/DC_using_OCR')\n",
    "\twith open('data.csv', 'w') as writeFile:\n",
    "\t\tprint('Writing data to file....')\n",
    "\t\tfor data in dataFile:\n",
    "\t\t\t#i=i+1\n",
    "\t\t\t#writer = pd.write_csv(writeFile)\n",
    "\t\t\t#writer.writerow(text)\n",
    "\t\t\tif data!='' or data != ' ':\n",
    "\t\t\t\twriteFile.write(\"%s\\n\" % data)\n",
    "\t\t#writeFile.write(\"%s\\n\\n\" % dataFile)\n",
    "\t\n",
    "\treturn dataFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting pdf document to images\n",
      "pdf to image conversion done\n",
      "['/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page3.tiff', '/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page0.tiff', '/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page7.tiff', '/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page8.tiff', '/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page5.tiff', '/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page2.tiff', '/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page4.tiff', '/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page6.tiff', '/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page1.tiff']\n",
      "Reading images.......!!!!\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page3.tiff\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page0.tiff\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page7.tiff\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page8.tiff\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page5.tiff\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page2.tiff\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page4.tiff\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page6.tiff\n",
      "/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir/The Yale cTAKES extensions for document.pdf-page1.tiff\n",
      "Printing data....\n",
      "['Downloaded from jamia.bmj.com on February 27, 2012 - Published by group.bmj.com  Research and applications  long-range detection and post-negation triggers. To address these issues, we replaced the cTAKES negation-detection algorithm with an annotator based on the latest version of the Java General NegEx package, which supports long-range detection and post-negation triggers.  In order to efficiently extract different feature sets from documents annotated with cTAKES, we developed a module that stores cCTAKES annotations in a relational database. UIMA annotations are limited in complexity and obey a strict class hierarchy. These restrictions on the structure of UIMA annota- tions facilitate a high-fidelity relational representation. We used an object-relational mapping tool (Hibernate) to map UIMA annotations to relational database tables using a table-per- subclass strategy; refer to the online appendix for a detailed description of the data model.?” YTEX supports SQL Server, Oracle, and MySQL databases. The effort involved in mapping new or modified annotations to the database is minimal, making this approach applicable to any UIMA annotation.  Storing annotations in a relational database greatly simplifies the development of rule-based classifiers: document feature vectors can be retrieved using SQL queries, and rules can be implemented using SQL ‘case’ statements.  Machine-learning document-classification techniques often employ the ‘bag-of-words’ or ‘term-document matrix’ repre- sentation of documents.” In this representation, documents occupy a feature space with one dimension for each word or term; words may be a word from a natural language or may be a technical identifier. The value of each dimension is typically either an indicator, asserting the presence of the word in the document, or a numeric value, indicating the term frequency. This feature space is typically high-dimensional and sparse, that is, the feature vectors mostly contain zeros. Most statistical packages support specialized file formats for efficient handling and exchange of sparse data sets. To use the bags-of-words document representation with WEKA, we developed a tool for exporting annotations obtained via SQL queries in the WEKA sparse file format. The tool takes as a parameter an SQL query that retrieves instance id, attribute name, and attribute value triples; it executes the query and rotates rows into columns to produce a sparse matrix representation of the data (figure 2). This transformation is similar to the SQL ‘pivot’ operator but differs in that it can create a matrix with an arbitrary number of columns.  The generic nature of the tool allows classification on any unit of text: the instance id can refer to a document, sentence, or phrase. The attribute name represents a dimension—for  SQL Query 1024 c0994163 1 1024 C0041618 3 Bag of Words 1024 1 3  Figure 2 Bag-of-Words Exporter pivots instance id, attribute name, attribute value triples into a sparse matrix.  616  example, a stemmed word or concept identifier; and the attri- bute value may be numeric or categorical. The tool enables the integration of other relational data sources with document annotation data—for example, administrative, pharmacy, or laboratory data. Refer to the online appendix for sample SQL statements used to export document annotations and adminis- trative data for use with WEKA.  Reference standard document corpus construction  To develop a gold-standard classification of radiographic findings indicative of hepatic decompensation, we used the results of a chart review designed to screen for and confirm cases of hepatic decompensation in the Veterans Aging Cohort Study (VACS) (figure 3). For the chart review, subjects enrolled in VACS were screened for radiographic findings of hepatic decompensation at enrollment by evaluating for suggestive International Classification of Diseases, Ninth Revision, Clinical Modification (ICD-9-CM) diagnostic codes, and laboratory abnormalities up to 1 year before through 6 months after entry into the cohort to identify possible prevalent cases. Additionally, a random sample of 100 patients who did not screen positive by the above criteria was selected to ensure the absence of hepatic decompensation events. Two trained data abstractors reviewed reports of abdominal ultrasounds, abdominal CT scans, and MRI studies, and recorded onto structured data- collection forms the following information: presence and quantity of ascites (fluid within the peritoneal cavity); presence and location of varices (dilated veins within the esophagus and stomach), and presence, number, and dimensions of liver masses. Two endpoint adjudicators with expertise in chronic liver diseases reviewed data forms and determined whether these outcomes of interest (ie, ascites, varices, liver masses) were present or absent. Disagreement on classification of the finding resulted in a review by a third reviewer to adjudicate the out- come. All findings were recorded in an electronic ‘adjudication database.’  As part of this study, we randomly selected the data- abstraction forms of 236 patients with ICD-9-CM diagnostic codes and/or laboratory abnormalities suggestive of hepatic decompensation and transcribed them to a database. We then linked the abstraction data to the original radiology reports, and defined a gold-standard classification of radiology reports. We labeled radiology reports included in the chart review ‘abdom- inal radiology reports.’ We assigned additional class labels to these reports indicating the presence of ascites, varices, and/or liver masses based on the data-abstraction forms (figure 4).                            Outcomes Adjudication       Liver  Cy Previous Chart Review  i Document Classification for this study  Figure 3. Development of the gold standard. ICD, International Classification of Diseases, Ninth Revision; VACS, Veterans Aging Cohort Study.  J Am Med Inform Assoc 2011;18:614—620. doi:10.1136/amiajnI-2011-000093', 'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/51175087  ResearchGate  The Yale cTAKES extensions for document classification: Architecture and  application  Article in Journal of the American Medical Informatics Association - May 2011  DOI: 10.1136/amiajnI-2011-000093 - Source: PubMed  CITATIONS 55  8 authors, including:  Vijay Garla Yale University  14 PUBLICATIONS 235 CITATIONS  SEE PROFILE  Zachariah Dorey-Stein Tulane University  13 PUBLICATIONS 270 CITATIONS  SEE PROFILE  Some of the authors of this publication are also working on these related projects:  poet Risk of Liver Complications with HIV/Hepatitis B View project  All content following this page was uploaded by Vijay Garla on 30 May 2014.  The user has requested enhancement of the downloaded file.  READS 327  Vincent Lo Re  University of Pennsylvania  147 PUBLICATIONS 2,385 CITATIONS  SEE PROFILE  Julie A Womack Yale University  51 PUBLICATIONS 772 CITATIONS  SEE PROFILE', 'Downloaded from jamia.bmj.com on February 27, 2012 - Published by group.bmj.com  2 = [=] s Ld  PSCC meme 0H  a  20. 21. 22.  620  UIMA. http://uima.apache.org/.  Cunningham H, Maynard D, Bontcheva K, et a/. An architecture for development of robust HLT applications. 40th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2002:168—75.  Aronson AR. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. Proc AMIA Symp 2001:17—21.  Langley P, Simon HA. Applicatons of machine learning and rule induction. Comm ACM 1995;38:55—64.  Wilcox A. Automated Classification of Medical Text Reports. PhD thesis. New York: Columbia University, 2000.  Crowley RS, Castine M, Mitchell K, et a/. caTIES: a grid based system for coding and retrieval of surgical pathology reports and tissue specimens in support of translational research. J Am Med Inform Assoc 2010;17:253—64.  Coden A, Savova G, Sominsky |, et a/. Automatically extracting cancer disease characteristics from pathology reports into a Disease Knowledge Representation Model. J Biomed Inform 2009;42:937—49.  Zeng QT, Goryachev S, Weiss S, et al. Extracting principal diagnosis, co-morbidity and smoking status for asthma research: evaluation of a natural-language-processing system. BMC Med Inform Decis Mak 2006;6:30.  Savova GK, Masanz JJ, Ogren PV, et a/. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. J Am Med Inform Assoc 2010;17:507—13.  Mark H, Eibe F, Geoffrey H, et a/. The WEKA data mining software: an update. SIGKDD Explor 2009;11:8.  Savova GK, Ogren PV, Duffy PH, et a/. Mayo clinic NLP system for patient smoking status identification. J Am Med Inform Assoc 2008;15:25—8.  Conway M, Doan S, Kawazoe A, et al. Classifying disease outbreak reports using n-grams and semantic features. nt J Med Inform 2009;78:E47—58.  D’Avolio LW, Nguyen TM, Farwell WR, et a/. Evaluation of a generalizable approach to clinical information retrieval using the automated retrieval console (ARC). J Am Med Inform Assoc 2010;17:375—82.  Wilcox A, Hripcsak G. Medical text representations for inductive learning. Proc AMIA Symp 2000:923—7.  Justice AC, Dombrowski E, Conigliaro J, et a/. Veterans Aging Cohort Study (VACS): Overview and description. Med Care 2006;44(8 Suppl 2):S13—24.  Sominsky I, Coden A, Tanenblatt M. CFE—A System for Testing, Evaluation and Machine Learning of Uima Based Applications. Morocco: LREC, 2008.  Salton G, McGill M. Introduction to Modern Information Retrieval. New York: McGraw Hill, 1983.  Lo Re Ill V, Bidwell GM, Lim JK, et al. A method to identify and confirm hepatic decompensation events in a Multicenter Cohort Study. Pharmacoepidemiology and Drug Safety. In press.  23. 24.  25.  26.  27:  28.  29.  30.  31.  32.  33.  34.  35.  37.  38.  39.  40.  41.  42.  43.  LVG. http://lexsrv3.nlm.nih.gov/LexSysGroup/Projects/lvg/2010/index.html. Chapman WW, Bridewell W, Hanbury P, et a/. A simple algorithm for identifying negated findings and diseases in discharge summaries. J Biomed Inform 2001;34:301—10.  openNLP Max€nt. http://maxent.sourceforge.net/.  National Library of Medicine. UMLS reference manual. http:/Avww.ncbi.nim.nih. gov/books/NBK9676/ (accessed 19 May 2011).  Solti 1. Negex: Negation Identification for Clinical Conditions, 2009. http://code. google.com/p/negex/.  Hibernate. /nheritance Mapping. http://docs jboss.org/hibernate/core/3.3/reference/ en/htm//inheritance.html.  Apte C, Damerau F, Weiss SM. Automated learning of decision rules for text categorization. ACM Trans Inform Syst 1994;12:233—51.  Quinlan JR. C4.5: Programs for Machine Learning. San Francisco: Morgan Kaufmann Publishers, 1993. ISBN: 9781558602380.  Breiman L. Random forests. Mach Learn 2001;45:5—32.  Yang H, Spasic |, Keane JA, et a/. A text mining approach to the prediction of disease status from clinical discharge summaries. J Am Med Inform Assoc 2009;16:596—600.  Patrick J, Li M. High accuracy information extraction of medication information from clinical notes: 2009 i2b2 medication extraction challenge. J Am Med Inform Assoc 2010;17:524—7.  Dumais §S, Platt J, Heckerman D, et a/. Inductive learning algorithms and representations for text categorization. Proceedings Of The Seventh International Conference On Information And Knowledge Management. Bethesda, MD: ACM, 1998:148—55.  van Rijsbergen CJ. Information Retrieval. 2nd edn. London: Butterworth, 1979. Zhou L, Hripcsak G. Temporal reasoning with medical data—a review with emphasis on medical natural language processing. J Biomed Inform 2007;40:183—202. Gasperin C, Briscoe T. Statistical Anaphora Resolution in Biomedical Texts. Manchester, UK: Coling 2008 Organizing Committee, 2008:257—64. Mykowiecka A, Marciniak M, Kups¢ A. Rule-based information extraction from patients’ clinical data. J Biomed Inform 2009;42:923—36.  Savova G, Bethard S, Styler W, et al. Towards temporal relation discovery from the clinical narrative. AMIA Annu Symp Proc 2009;2009:568—72.  Hahn U, Romacker M, Schulz S. MedSynDiKATe—design considerations for an ontology-based medical text understanding system. Proc AMIA Symp 2000:330—4. Roberts A, Gaizauskas R, Hepple M, et a/. Building a semantically annotated corpus of clinical texts. J Biomed Inform 2009;42:950—66.  Wilcox AB, Hripcsak G. The role of domain knowledge in automating medical text report classification. J Am Med Inform Assoc 2003;10:330—8.  Garla V. YTEX: Yale cTAKES Extensions, 2010. http://code.google.com/p/ytex/.  J Am Med Inform Assoc 2011;18:614—620. doi:10.1136/amiajnI-2011-000093', 'Downloaded from jamia.bmj.com on February 27, 2012 - Published by group.bmj.com  The Yale cTAKES extensions for document  JAMIA classification: architecture and application  Vijay Garla, Vincent Lo Re III, Zachariah Dorey-Stein, et al.  J Am Med Inform Assoc 2011 18: 614-620 originally published online May 27, 2011  doi: 10.1136/amiajnI-2011-000093     Updated information and services can be found at: http://jamia.bmj.com/content/18/5/614.full.html     These include:  References This article cites 22 articles, 8 of which can be accessed free at: http://jamia.bmj.com/content/18/5/614.full.html#ref-list-1  Article cited in: http://jamia.bmj.com/content/18/5/614. full.htmli#related-urls  Email alerting Receive free email alerts when new articles cite this article. Sign up in service the box at the top right corner of the online article.  Notes     To request permissions go to: http://group.bmj.com/group/rights-licensing/permissions  To order reprints go to: http://journals.bmj.com/cgi/reprintform  To subscribe to BMJ go to: http://group.bmj.com/subscribe/', 'Downloaded from jamia.bmj.com on February 27, 2012 - Published by group.bmj.com  Research and applications  Some clinical concepts consisted of non-contiguous tokens, making them difficult to capture in a dictionary. For example, the following phrases were used to note the presence of ascites in radiology reports: ‘fluid is noted in the subhepatic area’, ‘free fluid around the liver is noted’, or ‘free fluid in the perihepatic region’. In these examples, the term ‘fluid’ is separated from the term ‘liver’ or ‘hepatic’ by several variable words. We configured regular expressions to identify these concepts.  Evaluation of machine-learning algorithms Although the accuracy of the rule-based classifiers was satis- factory, we explored whether machine-learning algorithms could improve classification accuracy by using additional features that we overlooked, or features that could not easily be used in simple rule-based classifiers. For example, radiology reports that asserted the presence of hepatocellular carcinoma often asserted the presence of liver masses; machine-learning algorithms may leverage such associations to improve classification accuracy. We trained and evaluated the following machine-learning algo- rithms: decision trees (C4.5 algorithm), machine-learning analogs of rule-based classifiers”’ °°; random forests, ensembles of decision trees\"; and SVMs, which have been successfully applied in document classification.* *°-™# To test whether system tuning and feature representation improved classifier performance, we evaluated classifiers against different representations of the document corpus: > baseline: this dataset represents the annotations generated by the un-tuned pipeline; > simple: this dataset employs a bag of affirmed terms document representation, which ignores document section and negated terms;  > rich: this dataset uses the rich document feature representa- tion that leverages the syntactic and negation context of named entities as described above.  We exported the document corpus in the WEKA sparse file format, split the corpus into a training set and a held-out test set, performed cross-validation on the training set, selected the optimal algorithm, and performed a final evaluation of classifier accuracy against the held-out test set. We used the cross-vali- dation results to estimate classifier accuracy for varices, as we did not have enough reports for a held-out test set. Refer to the online appendix for a detailed description of the different corpus representations and machine learning process.  The datasets we exported had over 4000 features. For feature selection, we ranked features from the training set by mutual information and evaluated classifier performance using a 4-fold  Table 1  cross-validation on the top n features, with n varying between 1 and 500. Accuracy peaked with fewer than 500 features for all classification tasks. We then performed a 4-fold cross-validation 25 times with the optimal algorithm and number of features on the training set to generate empirical distributions for the information retrieval metrics specificity, precision, recall, and F,-Score with which we assess classifier performance. These are defined as follows”:  specificity: TN/(IN+FP);  precision (positive predictive value): P=TP/(IP+FP);  recall (sensitivity): R=TP/(TP+FN);  F,-score: (2*P*R)/(P  R);  TP: true positives (classified as positive when in fact positive);  FP: false positives (classified as positive when in fact negative); TN: true negatives (classified as negative when in fact negative); EN: false negatives (classified as negative when in fact positive).  RESULTS  Cross-validation results  Classifiers trained on the tuned dataset that employed the rich feature representation performed significantly better than clas- sifiers trained on the untuned dataset, and the dataset based on a simple feature representation (table 1). An exception was varices, in which classifiers trained on the simple feature repre- sentation performed best; this difference was however not statistically significant (p=0.0157). These results show that tuning NER, negation detection, and optimizing the feature representation significantly improved classifier performance.  On the rich data set, classifiers achieved optimum perfor- mance with only the features used by our rule-based classifiers; additional features did not add predictive power to the classifier. The decision trees ‘learned’ from the rich dataset were similar or identical to the rule-based classifiers. Because of their similarity to machine-learned trees, we did not explicitly evaluate the performance of the rule-based classifiers.  On the rich dataset, simple decision trees using few features achieved optimal performance. For the identification of abdominal radiology reports and liver masses, classifiers trained on other datasets required more features and the more complex random forest and SVM algorithms to attain optimal performance.  Performance on test set For each classification task, we selected the best classifier and evaluated it against the held-out test set (table 2).  Classifier parameters, information retrieval scores, and probability mean of the simple/baseline F,-score, and how they differ from rich F,-scores        Task Dataset _— Classifier Features Specificity Precision Recall F,-score _—p Value Abdominal Rich Tree 10 0.997 0.996 0.997 0.997 Radiology Baseline Svm 75 0.994 0.960 0.928 0.944* 0 Reports Simple Svm 30 0.991 0.938 0.939 0.938* 0 Ascites Rich Tree 1 0.983 0.928 0.939 0.932 Baseline Tree 1 0.976 0.895 0.893 0.893* 0 Simple Tree 1 0.962 0.855 0.989 0.916* 0.0047 Liver masses Rich Tree 2 0.979 0.830 0.781 0.800 Baseline Tree 8 0.975 0.782 0.696 0.728* 0 Simple Random forest 125 0.970 0.710 0.528 0.595* 0 Varices Rich Tree 1 0.995 0.894 0.94 0.911 Baseline  Svm 4 0.992 0.863 0.905 0.871* 0.0033 Simple Tree 1 0.995 0.904 1.000 0.946 0.0157     *Significant at 0.01 level.  618  J Am Med Inform Assoc 2011;18:614—620. doi:10.1136/amiajnI-2011-000093', 'Downloaded from jamia.bmj.com on February 27, 2012 - Published by group.bmj.com  Research and applications  previously been deployed within the VHA. \\' !? The cTAKES stores annotations in the UIMA Common Analysis Structure (CAS), a structured object graph superimposed over the unstructured document text. One method for extracting features is the development of a CAS Consumer, a custom software component that accesses the CAS and exports anno- tations in a user-specified format. However, it is impractical to write software for each set of desired features. The Mayo Weka/ UIMA Integration (MAWUI) library provides tools for exporting data from applications based on UIMA for use with the Weka machine-learning environment.\\'? MAWUI requires the imple- mentation of custom software components by the user to extract features, and thus suffers from the same limitations as the CAS Consumer. Another alternative for the extraction of UIMA annotations is the Common Feature Extraction System (CFE), which enables the declarative extraction of data from the CAS using an XML-based Feature Extraction Specification Language.\" One limitation of the CFE is that it cannot perform aggregate calculations on document features; for example, the CFE cannot output the number of times each term occurs within a document, a commonly used feature in document classification.’ The Automated Retrieval Console (ARC) is a clinical document classification system based on cTAKES.\\'” The ARC is designed to enable end users with little knowledge of NLP or machine learning to develop document classifiers; it does this by training machine-learning algorithms on various combinations of cTAKES annotations to classify documents. The ARC does not support functionality that more advanced users require, such as rule-based classifier development, manual feature selection, and customizable feature representa- tion. A further limitation is that the gold standard document corpus must be curated within the ARC user interface; this is incompatible with the typical chart review process, which requires the review of thousands of notes and the storage of chart abstraction data in a format amenable to subsequent analyses. A limitation of both the CFE and ARC is their inability to integrate other structured data sources such as administrative data, pharmacy, and laboratory values with the document representation.  To simplify feature extraction and classifier development, we extended cTAKES to store document annotations in a relational database. This approach enabled seamless integration with other structured data sources stored in relational databases; enabled the development of rule-based classifiers using the structured query language (SQL); and enabled the declarative extraction of document annotations for use with WEKA, simplifying the development of machine-learning based classifiers.  0  DESIGN OBJECTIVES  We evaluated the ease of document classifier development by applying our system to a document corpus derived from a chart review to screen for and confirm cases of hepatic decompensa- tion in VACS.” As part of this chart review, trained abstractors reviewed over 13000 radiology reports from over 395 patients; fewer than 400 reports asserted the presence of a clinical condition indicative of hepatic decompensation. Our objective for this use case was to quickly develop classifiers that accurately identified these reports, thereby dramatically reducing the effort involved in future screens of hepatic decompensation in the VACS study. Abstractors would still have to review the auto- matically identified reports to extract needed information; therefore, our goal was to develop document classifiers with high recall (sensitivity)—greater than 90%—and_ acceptable precision (positive predictive value)—greater than 80%.  J Am Med Inform Assoc 2011;18:614—620. doi:10.1136/amiajnI-2011-000093  METHODS  We extended the cTAKES pipeline to improve NLP capabilities, simplify feature extraction, and facilitate document classifier development. We constructed a gold standard document corpus of radiology reports suggestive of hepatic decompensation. We then applied the system as follows: (1) developed rule-based classifiers, (2) performed system tuning in which we iteratively improved document annotation by modifying the system configuration, and (3) evaluated machine-learning algorithms for document classification.  YTEX pipeline  The cTAKES is a modular pipeline of annotators that combines rule-based and machine-learning techniques to annotate syntactic constructs, named entities, and their negation context in clinical text. CTAKES uses the OpenNLP Maximum Entropy package for sentence detection, tokenizing, part-of-speech tagging, and chunking; uses the SPECIALIST lexical variant generator for stemming; and uses an algorithm based on NegEx for negation detection.2*-*° The cTAKES DictionaryLookup module performs named entity recognition by matching spans of text to entries from a dictionary. We used the cTAKES distribution included with ARC, which is distributed with Unified Medical Language System (UMLS) database tables for use with the DictionaryLookup module.?° The UMLS Meta- thesaurus unifies over 100 source vocabularies and assigns each term a concept unique identifier (CUI).  We modified cTAKES as follows: we developed regular- expression-based named entity recognition and section detection annotators (NamedEntityRegex and SegmentRegex); we adapted the latest version of the NegEx algorithm to cTAKES for negation detection (Negex); and we developed a module to store annotations in a relational database (DBConsumer; see figure 1). The annotators we developed are highly configurable; refer to the online appendix for a detailed description of all modifications to the cTAKES pipeline and configurations used in this study.  cTAKES can annotate demarcated sections from documents that conform to the Clinical Document Architecture format, which is not used in the VHA. To identify document sections, we developed an annotator that identifies section headings and boundaries based on regular expressions.  The DictionaryLookup algorithm performs named entity recognition by matching spans of document text to word sequences from a dictionary. Some clinical concepts are too complex, have too many lexical variants, or consist of non- contiguous tokens, making them difficult to represent in a simple dictionary. To address this issue, we developed an annotator that uses regular expressions to identify such concepts.  The cTAKES negation-detection algorithm is based on an older version of the NegEx algorithm and has limited support for     Dictionary >) —  Figure 1 Yale clinical Text Analysis and Knowledge Extraction System Extensions (YTEX) pipeline. New annotators developed as part of this study are shaded in gray. DB, database.     615', 'Downloaded from jamia.bmj.com on February 27, 2012 - Published by group.bmj.com                        All Radiology: 395 Patients 13,859 Reports  Gold Standard: 236 Patients 8,091 Reports  Figure 4 Dimensions of the document corpus.  Rule-based classifier development  We initially classified documents using manually developed rules. These interpretable classifiers allowed us to explore the feature space, optimize feature representations, and understand and rectify NLP errors that caused misclassification. We imple- mented the rules as SOL case statements, operating on feature vectors retrieved via SQL queries. For example, to identify radi- ology reports that assert the presence of varices, we focused on named entity annotations that contain CUIs related to varices, and represented documents as vectors with a column for each concept. Refer to the online appendix for sample SQL state- ments and a list of features we used in classification rules.  The ability to filter, aggregate, and transform document annotations using SQL queries allowed us to easily experiment with different representations of document concepts and their semantic and syntactic context. We found the following feature selection and representation approaches effective: filtering out concepts located within certain document sections; representing the negation status of concepts using a ‘relative negation count’; combining different concepts in a single feature; and using within-sentence concept co-occurrence.  The document section to which a term belongs is an impor- tant feature for document classification: for the discrimination of abdominal radiology reports from other radiology reports, terms in the title had far more importance than terms in the document body. For the identification of documents that assert the presence of a clinical condition (ie, ascites, varices, or liver masses), we found that filtering out terms from the clinical history section of documents improved classifier performance.  We combined distinct UMLS concepts under a single feature, thereby reducing the number of features needed and simplifying rule development. For example, the distinct UMLS concepts ‘Ascites’ (C0003962), ‘Peritoneal Fluid’ (C0003964), and intra- abdominal collection (C0401020) could for the purposes of this classification task be grouped under a single feature ‘Ascites.’  For the identification of liver masses, within-sentence co- occurrence was an important feature. For example, the sentence ‘A rounded, echogenic focus is seen in the left lobe of the liver’ contains the terms ‘echogenic focus’ and ‘liver’. We used co- occurrence of these terms within a sentence as a simple heuristic to infer the presence of a liver mass. Knowing that both these terms are in the same document is insufficient to infer the presence of a liver mass.  Concepts can be negated and affirmed within the same document as a result of errors in the negation detection  J Am Med Inform Assoc 2011;18:614—620. doi:10.1136/amiajnI-2011-000093     PSC r mene yt  algorithm, or due to deeper semantic content; exclusively considering affirmed or negated terms obscures this information. To address this issue, we represented the negation context of concepts using a ‘relative affirmation count’: the number of times a concept was affirmed minus the number of times it was negated within a document.  For example, the rule for the classification of varices compares the number of affirmed varices terms to the number of negated varices terms outside of the ‘Clinical History’ section of the document (figure 5). If any particular varices term is affirmed more than negated, the document is classified as ‘varices positive.’ Refer to the online appendix for a description of other rule-based classifiers.  System tuning  To improve classifier performance, we performed multiple iter- ations of system tuning: (1) we generated document annota- tions with YTEX; (2) we classified documents using rule-based classifiers; (3) we manually examined all misclassified docu- ments, and modified rules to resolve misclassification errors where necessary; (4) we reconfigured YTEX to rectify NLP errors; (5) we forwarded incorrectly labeled radiology reports to endpoints arbitrators (VLR and ZD-S) who reviewed these documents and updated document labels and patient adjudication databases.  We found that many classification errors were due to prob- lems in named entity recognition (NER) and negation detection. To address these issues, we reconfigured the NER and negation- detection modules: we added entries to the dictionary used by the DictionaryLookup module; we configured regular expres- sions for use with the NamedEntityRegex module; and we modified the list of negation triggers used by the NegEx module. Refer to the online appendix for a detailed description of the regular expressions, dictionary entries, and negation triggers used for this study.  Upon evaluation of misclassified documents, we noticed that lexical variants of clinical concepts needed for classification were not included in the UMLS. For example, ultrasounds were often denoted with the term ‘echogram,’ which is not contained in the UMLS. We added additional entries to the YTEX UMLS dictionary to identify these concepts.         Varices Affirmed > Negated  Varices Esophogeal Varices Positive Affirmed > Negated  Gastric Varices Affirmed > Negated  in| Varices  Varices Positive Negative  Figure 5 Varices classification rule. If any one of the varices terms is affirmed more than it is negated, the tree assigns the document the class label ‘varices positive.’                                        Positive          617', 'Downloaded from jamia.bmj.com on February 27, 2012 - Published by group.bmj.com  Research and applications  Table 2 Classifier performance on test set     Classification task Specificity Precision Recall F,-score Abdominal radiology reports 0.997 0.950 0.976 0.963 Ascites 0.981 0.896 0.915 0.905  Liver masses 0.982 0.735 0.862 0.794  DISCUSSION  We achieved the goal of rapidly developing accurate document classifiers to facilitate the hepatic decompensation chart review. The extensions we developed greatly simplified the development of rule and machine-learning based document classifiers, allowing us to complete classifier development in a total of five man days. This included developing rule-based classifiers, tuning the NLP system, and training and evaluating machine-learning classifiers. The system we developed classified radiology reports from eight VHA sites with high accuracy; we were able to meet the goals of >90% recall and >80% precision for the classifica- tion of abdominal radiology reports and varices; the system, however, failed to meet these goals for the classification of reports that assert the presence of liver masses. To achieve these results for this use case, we tuned named entity recognition and negation detection, and explored various feature combinations. Interpretable rule-based classifiers simplified tuning the NLP pipeline and exploring feature representations.  Through an examination of misclassified documents, we recognized issues where further work is required. Common causes of misclassification included lack of temporal context, lack of location context, and lack of pronominal anaphora or co- reference resolution; refer to the online appendix for a detailed discussion and examples of misclassification. These issues are the focus of active research in the clinical NLP field®°*!; we will address these issues in future work.  Tuning named entity recognition (NER) and negation detec- tion significantly improved classifier performance: this is demonstrated by the relative performance of classifiers evaluated on annotations derived from the tuned and untuned pipelines (rich vs baseline). We improved NER by adding lexical variants of clinical concepts to the dictionary, and by using regular expressions to identify clinical concepts. We used the NegEx algorithm for negation detection, and updated the default negation triggers. The resulting system was optimized for clas- sification tasks specific to this study. Evaluations of cTAKES estimate the F,-score of NER to be 0.824\\'°; thus, in general, it may be necessary to tune NER for specific classification tasks. Our tuning approach is generally applicable, and can be used to optimize NER and negation detection for other problem domains.  Choosing an optimal feature representation significantly improved classification performance. Text classification systems have used combinations of words, phrases, word sense, UMLS/ SNOMED concepts, and others; no single feature representation is optimal for all document classification tasks.\\'°~-\\'® ” Finding the optimal representation for a given classification task requires exploration and experimentation with multiple feature repre- sentations. Domain knowledge can be incorporated in the feature representation via feature selection, and by combining multiple features to create new variables.’” We simplified this process by storing document annotations in a relational data- base, allowing us to efficiently explore the feature space and optimize the feature representation.  Storing annotations in a relational database also greatly simplified the development of both rule and machine-learning  J Am Med Inform Assoc 2011;18:614—620. doi:10.1136/amiajnI-2011-000093  based classifiers. The relational representation inherently supports the rule-based document classification approach: we implemented classification rules as SQL case statements, oper- ating on feature vectors retrieved via SQL queries. To support machine-learning approaches, we developed highly configurable tools to extract document features from the database in a bag-of-words representation for use with the WEKA toolkit.  We applied the lessons learned from this study to other chart reviews within VACS. The most laborious step of this study was the construction of a gold standard document corpus: this required the transcription of chart abstraction data from paper forms to a database, and linking these data to specific radiology reports. In general, the information captured as part of medical chart reviews is insufficient to construct a gold-standard corpus: the structured data produced by chart reviews typically synthesizes findings from multiple notes in the patient chart. However, in order to automate document classification and information extraction, notes must be linked to the information extracted from them. To address this issue, we have developed databases that integrate the chart-review data-abstraction process with manual document annotation, yielding gold stan- dard corpora that we can use to develop document classifiers. We have developed databases for chart reviews to confirm cases of cancer from pathology, progress, and radiology notes; to study cases of community acquired pneumonia from microbiology, radiology, and progress notes; and to identify homeless veterans from progress notes. Future work will focus on developing document classifiers to assist these studies.  CONCLUSIONS  cTAKES is a comprehensive clinical NLP system that serves as a foundation for the development of clinical document classifi- cation and information-extraction systems. The Yale cTAKES Extensions (YTEX) simplify feature extraction and the devel- opment of rule and machine-learning based classifiers. We have released YTEX as open source.\"  We used these tools to develop document classifiers that identify radiology reports with findings suggestive of hepatic decompensation. YTEX enabled us to efficiently explore the feature space; create a feature representation that leverages domain knowledge, the syntactic structure of the documents, and the negation context of concepts; and quickly develop rule and machine-learning based classifiers. In the future, we will apply these tools to identify reports relevant to medical chart reviews performed as part of other VACS studies.  Funding Yale School of Medicine (VG). VA grant HIR 08-374 HSR&D: Consortium for Health Informatics (CB, MS). VA Office, Academic Affiliations, Information Research & Development (Medical Informatics Fellowship Program) (JW, CB, Ad). National  Institute on Alcohol Abuse and Alcoholism (U10 AA 13566) (Au, Pl; CB, FK). National Institute of Allergy and Infectious Diseases (KO1 Al 070001; VLR).  Competing interests None.  Provenance and peer review Not commissioned; externally peer reviewed.  REFERENCES  1. Bates DW, Kuperman GJ, Wang S, et a/. Ten commandments for effective clinical decision support: making the practice of evidence-based medicine a reality. J Am Med Inform Assoc 2003;10:523—30.  2. Justice AC, Erdos J, Brandt C, et a/. The veterans affairs healthcare system:  a unique laboratory for observational and interventional research. Med Care 2006;44 (8 Suppl 2):S7—12.  3. Joachims T. Text Categorization With Support Vector Machines: Learning With Many Relevant Features. Computer Science Department of the University of Dortmund;1998, Research Reports of the unit no. Vill (Al), Report 23. 137—42.  4. Hayes PJ, Weinstein SP. CONSTRUE/TIS: a system for content-based indexing of a database of news stories. Proceedings of the The Second Conference on Innovative Applications of Artificial Intelligence. AAAI Press, 1991:49—64.  619', \"Downloaded from jamia.bmj.com on February 27, 2012 - Published by group.bmj.com  Research and applications  The Yale cTAKES extensions for document classification: architecture and application  Vijay Garla,’ Vincent Lo Re Ill,? Zachariah Dorey-Stein,? Farah Kidwai,? Matthew Scotch,?* Julie Womack,?° Amy Justice,?° Cynthia Brandt®’  » Additional appendices are published online only. To view these files please visit the journal online (www. jamia.org).  ‘Interdepartmental Program in Computational Biology & Bioinformatics, Yale University, New Haven, Connecticut, USA Center for Clinical Epidemiology and Biostatistics, University of Pennsylvania School of Medicine, Philadelphia, Pennsylvania, USA “Connecticut VA Healthcare System, West Haven, Connecticut, USA  ‘Department of Biomedical Informatics, Arizona State University, Tempe, Arizona, USA 5Yale University School of Nursing, New Haven, Connecticut, USA  °General Internal Medicine, Yale University School of Medicine, New Haven, Connecticut, USA ’Yale Center of Medical Informatics, Yale University School of Medicine, New Haven, Connecticut, USA  Correspondence to  Vijay Garla, Yale Center for Medical Informatics, PO Box 208009, New Haven, CT 06520-8009, USA; vijay.garla@yale.edu  The views expressed in this article are those of the authors and do not necessarily reflect the position or policy of the Department of Veterans Affairs.  Received 9 December 2010 Accepted 22 April 2011 Published Online First  27 May 2011  614  ABSTRACT  Background Open-source clinical natural-language- processing (NLP) systems have lowered the barrier to the development of effective clinical document classification systems. Clinical natural-language- processing systems annotate the syntax and semantics of clinical text; however, feature extraction and representation for document classification pose technical challenges.  Methods The authors developed extensions to the clinical Text Analysis and Knowledge Extraction System (cTAKES) that simplify feature extraction, experimentation with various feature representations, and the development of both rule and machine-learning based document classifiers. The authors describe and evaluate their system, the Yale cTAKES Extensions (YTEX), on the classification of radiology reports that contain findings suggestive of hepatic decompensation. Results and discussion The F,-Score of the system for the retrieval of abdominal radiology reports was 96%, and was 79%, 91%, and 95% for the presence of liver masses, ascites, and varices, respectively. The authors released YTEX as open source, available at http://code. google.com/p/ytex.  INTRODUCTION  The rich clinical data stored in the electronic medical record are important to clinical-decision support, comparative effectiveness research, and epidemiological and clinical research studies.! ? The electronic medical record stores much of the rele- vant information in the form of unstructured free text. Automated document classification and information-extraction techniques are the keys to accessing the clinical data locked in unstructured text.  Methods for automated document classification include rule-based and machine-learning  tech- niques.° * In the rule-based approach, experts manually define classification rules. In the machine- learning approach, algorithms construct classifiers automatically using training data. Clinical natural language processing (NLP) systems annotate syntactic structure and semantic content within clinical text, and typically store annotations in a hierarchical data structure.>~’ In contrast, rule and machine-learning classifiers typically operate on ‘flat’ feature vectors. Converting between the hierarchical document representation output by NLP systems and the flat feature space required by classifiers is one of the most time-consuming and labor-intensive tasks in the development process, and one of the most important: the power  of machine-learning algorithms depends on the construction of a feature representation that makes learning tractable.° ? Our goal was to extend an open-source clinical NLP system to simplify feature extraction and the development of rule and machine-learning based document-classification systems.  BACKGROUND Historically, clinical NLP systems were built for specific healthcare systems, focused on specific goals, and involved a large implementation effort. Recently, open-source clinical NLP systems based on modular frameworks have become available, dramatically lowering the amount of resources and level of expertise needed to develop effective clinical document-classification systems. These include the clinical Text Analysis and Knowledge Extrac- tion System (cTAKES), the Medical Knowledge Analysis Tool (MedKAT/P), Health Information Text Extraction (HITEx), and the Cancer Text Information Extraction System (caTIES).'°~!% The open-source WEKA data-mining toolkit has been used in conjunction with these systems to develop machine-learning based text classifiers.’? 4 1°  These systems annotate syntactic structures such as sections, sentences, phrases (chunks), tokens (words), and their part-of-speech; perform named entity recognition and map spans of text to concepts from a controlled vocabulary or ontology; and identify the negation context of named enti- ties. Different combinations of text annotations may be appropriate for different classification tasks, and finding the optimal feature Tepresentation is critical to classifier development. !°~'°  The application motivating this study was the automation of document classification in the Veterans Aging Cohort Study (VACS), an ongoing, prospective cohort study that follows HIV-infected and demographically similar HIV-uninfected veterans receiving medical care at eight Veterans Health Administration (VHA) facilities.!? Central to many VACS projects are medical chart reviews, the objective of which is to extract a well-defined set of information from a specific subset of medical records. Automated document classification can facilitate this process by identifying the reports relevant to the chart review. We sought to deploy and extend an open-source clinical NLP system and develop a methodology for the rapid implementa- tion of document classifiers to facilitate VACS chart reviews.  We selected the cTAKES, a comprehensive clin- ical NLP system based on the Unstructured Infor- mation Management Architecture (UIMA) that has     J Am Med Inform Assoc 2011;18:614—620. doi:10.1136/amiajnI-2011-000093\"]\n",
      "Writing data to file....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\tfolder=\"/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/pdfs\"\n",
    "\ttargetFolder=\"/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/imagesDir\"\n",
    "\timages = []\n",
    "\ti=0\n",
    "\t\n",
    "\t#imPath = \"/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/fwh.jpg\"\n",
    "\t#images = [cv2.imread(file) for file in files]\n",
    "\t\n",
    "\tpdfs_to_image(folder)\n",
    "\t\n",
    "\tfor filename in os.listdir(targetFolder):\n",
    "\t\timg = os.path.join(targetFolder,filename)\n",
    "\t\tif img is not None:\n",
    "\t\t\timages.append(img)\n",
    "\tprint(images)\n",
    "\treadContent=image_to_text(images)\n",
    "\n",
    "\tprint(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
