{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching data from sklearn directory\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(twenty_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer: \n",
      "  (0, 86580)\t1\n",
      "  (0, 128420)\t1\n",
      "  (0, 35983)\t1\n",
      "  (0, 35187)\t1\n",
      "  (0, 66098)\t1\n",
      "  (0, 114428)\t1\n",
      "  (0, 78955)\t1\n",
      "  (0, 94362)\t1\n",
      "  (0, 76722)\t1\n",
      "  (0, 57308)\t1\n",
      "  (0, 62221)\t1\n",
      "  (0, 128402)\t2\n",
      "  (0, 67156)\t1\n",
      "  (0, 123989)\t1\n",
      "  (0, 90252)\t1\n",
      "  (0, 63363)\t1\n",
      "  (0, 78784)\t1\n",
      "  (0, 96144)\t1\n",
      "  (0, 128026)\t1\n",
      "  (0, 109271)\t1\n",
      "  (0, 51730)\t1\n",
      "  (0, 86001)\t1\n",
      "  (0, 83256)\t1\n",
      "  (0, 113986)\t1\n",
      "  (0, 37565)\t1\n",
      "  :\t:\n",
      "  (11313, 87626)\t1\n",
      "  (11313, 30044)\t1\n",
      "  (11313, 76377)\t1\n",
      "  (11313, 119714)\t1\n",
      "  (11313, 47982)\t1\n",
      "  (11313, 28146)\t2\n",
      "  (11313, 88363)\t2\n",
      "  (11313, 56283)\t1\n",
      "  (11313, 111695)\t1\n",
      "  (11313, 90252)\t1\n",
      "  (11313, 51730)\t1\n",
      "  (11313, 68766)\t1\n",
      "  (11313, 89860)\t1\n",
      "  (11313, 80638)\t1\n",
      "  (11313, 4605)\t1\n",
      "  (11313, 76032)\t1\n",
      "  (11313, 89362)\t1\n",
      "  (11313, 90379)\t1\n",
      "  (11313, 64095)\t1\n",
      "  (11313, 95162)\t1\n",
      "  (11313, 87620)\t1\n",
      "  (11313, 111322)\t1\n",
      "  (11313, 85354)\t1\n",
      "  (11313, 50527)\t2\n",
      "  (11313, 56979)\t2\n"
     ]
    }
   ],
   "source": [
    "#count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "print('Count vectorizer: ')\n",
    "print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfIdf :\n",
      "  (0, 128420)\t0.04278499079283093\n",
      "  (0, 128402)\t0.05922294083277842\n",
      "  (0, 128026)\t0.060622095889758885\n",
      "  (0, 124931)\t0.08882569909852546\n",
      "  (0, 124031)\t0.10798795154169122\n",
      "  (0, 123989)\t0.08207027465330353\n",
      "  (0, 123984)\t0.036854292634593756\n",
      "  (0, 123796)\t0.049437556160455476\n",
      "  (0, 123292)\t0.14534718515938805\n",
      "  (0, 123162)\t0.2597090245735688\n",
      "  (0, 118983)\t0.037085978050619146\n",
      "  (0, 118280)\t0.2118680720828169\n",
      "  (0, 115475)\t0.042472629883573\n",
      "  (0, 114731)\t0.14447275512784058\n",
      "  (0, 114688)\t0.06214070986309586\n",
      "  (0, 114579)\t0.03671830826216751\n",
      "  (0, 114455)\t0.12287762616208957\n",
      "  (0, 114428)\t0.05511105154696676\n",
      "  (0, 113986)\t0.17691750674853082\n",
      "  (0, 111322)\t0.01915671802495043\n",
      "  (0, 109581)\t0.10809248404447917\n",
      "  (0, 109271)\t0.10844724822064673\n",
      "  (0, 108252)\t0.07526015712540636\n",
      "  (0, 106116)\t0.09869734624201922\n",
      "  (0, 104813)\t0.08462829788929047\n",
      "  :\t:\n",
      "  (11313, 62696)\t0.06213004660468942\n",
      "  (11313, 60910)\t0.34638730155641734\n",
      "  (11313, 60803)\t0.07995422310508192\n",
      "  (11313, 56979)\t0.039703068357897435\n",
      "  (11313, 56283)\t0.02607488632151599\n",
      "  (11313, 55411)\t0.06186613753683744\n",
      "  (11313, 51730)\t0.10067098834752666\n",
      "  (11313, 50527)\t0.05659515244000391\n",
      "  (11313, 47982)\t0.04878764010149915\n",
      "  (11313, 38412)\t0.1036626622028617\n",
      "  (11313, 38329)\t0.3188672316645619\n",
      "  (11313, 38325)\t0.15275412572672764\n",
      "  (11313, 37469)\t0.20012648171635533\n",
      "  (11313, 37413)\t0.16268391686660966\n",
      "  (11313, 33941)\t0.09553399237711814\n",
      "  (11313, 32988)\t0.06639443458516447\n",
      "  (11313, 31386)\t0.11766332475169258\n",
      "  (11313, 30044)\t0.03581554412880591\n",
      "  (11313, 28146)\t0.04703946070749562\n",
      "  (11313, 27696)\t0.1447050212747473\n",
      "  (11313, 25402)\t0.07399132052611829\n",
      "  (11313, 11390)\t0.14229394541348833\n",
      "  (11313, 8653)\t0.19138342755986237\n",
      "  (11313, 4605)\t0.06562288156075428\n",
      "  (11313, 3411)\t0.06958305141739549\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "print('TfIdf :')\n",
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training models \n",
    "# Naive Bayes (NB)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using NB\n",
      "0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "#predicting on test data\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print('Accuracy using NB')\n",
    "print(np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming Code\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying stemming and removing stop words\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
    "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after using NLTK\n",
      "0.8167817312798725\n"
     ]
    }
   ],
   "source": [
    "#predicting on the test data\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "print(\"Accuracy after using NLTK\")\n",
    "print(np.mean(predicted_mnb_stemmed == twenty_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 15  6 15 15 11 15 15 15 15]\n"
     ]
    }
   ],
   "source": [
    "# using the classifier model on real world daily news\n",
    "import os\n",
    "data1='/home/ikscare/Documents/Projects/Mousam/DC_using_OCR/newsData'\n",
    "newslist=os.listdir(data1)\n",
    "predi=text_mnb_stemmed.predict(newslist)\n",
    "print(predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 04.txt is of category 9\n",
      "Document 01.txt is of category 15\n",
      "Document 10.txt is of category 6\n",
      "Document 07.txt is of category 15\n",
      "Document 03.txt is of category 15\n",
      "Document 06.txt is of category 11\n",
      "Document 08.txt is of category 15\n",
      "Document 05.txt is of category 15\n",
      "Document 02.txt is of category 15\n",
      "Document 09.txt is of category 15\n"
     ]
    }
   ],
   "source": [
    "# showing formatted output\n",
    "i=0\n",
    "for pred in predi:\n",
    "    print('Document {0:s} is of category %d'.format(newslist[i]) %pred)\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
